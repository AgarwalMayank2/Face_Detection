{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "svIt5l3bdrY_"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9Lw07VP6or2OLSk0ne1kY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgarwalMayank2/Face_Detection/blob/KNN_Tavishi/KNN_Image_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "svIt5l3bdrY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiHXMskx2ojl",
        "outputId": "7546a7ce-8631-4bd2-9c96-52065fa48aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/lfw-dataset?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112M/112M [00:01<00:00, 103MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcRn43vE595y",
        "outputId": "bd054b86-f3b8-4395-89f6-2077718a19ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv(\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/people.csv\")\n",
        "print(df1.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj4CBE4u3ACL",
        "outputId": "21ca10bd-d882-40ed-b7d7-9f35506836c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 name  images\n",
            "0  Abdel_Madi_Shabneh     1.0\n",
            "1        Abdul_Rahman     1.0\n",
            "2        Abel_Pacheco     4.0\n",
            "3        Adriana_Lima     1.0\n",
            "4         Afton_Smith     1.0\n",
            "5        Ahmad_Jbarah     1.0\n",
            "6      Akhmed_Zakayev     3.0\n",
            "7     Alan_Dershowitz     1.0\n",
            "8   Alanis_Morissette     1.0\n",
            "9    Alberto_Gonzales     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=pd.read_csv(\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/peopleDevTrain.csv\")\n",
        "print(df2.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUTuNnWc5iZ3",
        "outputId": "5d340a0f-7290-4186-b754-182523d9111e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  name  images\n",
            "0              AJ_Cook       1\n",
            "1        Aaron_Eckhart       1\n",
            "2      Aaron_Patterson       1\n",
            "3        Aaron_Peirsol       4\n",
            "4           Aaron_Pena       1\n",
            "5         Aaron_Sorkin       2\n",
            "6     Abbas_Kiarostami       1\n",
            "7  Abdel_Aziz_Al-Hakim       1\n",
            "8   Abdel_Madi_Shabneh       1\n",
            "9  Abdel_Nasser_Assidi       2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3=pd.read_csv(\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/peopleDevTest.csv\")\n",
        "print(df3.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAlva0J16eIw",
        "outputId": "d52a330d-b065-4eea-ada6-3a73c993fbd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           name  images\n",
            "0                      AJ_Lamas       1\n",
            "1                   Aaron_Guiel       1\n",
            "2                  Aaron_Tippin       1\n",
            "3                     Abba_Eban       1\n",
            "4        Abdul_Majeed_Shobokshi       1\n",
            "5             Abdulaziz_Kamilov       1\n",
            "6                  Abdullah_Gul      19\n",
            "7              Abdullatif_Sener       2\n",
            "8                  Abel_Aguilar       1\n",
            "9  Abid_Hamid_Mahmud_Al-Tikriti       3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "images_address=\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled\"\n",
        "labels=os.listdir(images_address)\n",
        "paths_of_images=[]\n",
        "for root, dirs, files in os.walk(images_address):\n",
        "    for file in files:\n",
        "        paths_of_images.append(os.path.join(root, file))\n",
        "\n",
        "print(paths_of_images[:5])"
      ],
      "metadata": {
        "id": "7rZ-bi5f7Mhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e41a2f9-1585-432f-ac6f-237d62e9abfd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled/Wesley_Clark/Wesley_Clark_0001.jpg', '/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled/Wesley_Clark/Wesley_Clark_0002.jpg', '/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled/Myung_Yang/Myung_Yang_0001.jpg', '/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled/Ronald_Brower/Ronald_Brower_0001.jpg', '/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/lfw-deepfunneled/lfw-deepfunneled/Lance_Armstrong/Lance_Armstrong_0006.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features using CNN"
      ],
      "metadata": {
        "id": "xj82_wJv_WTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "naPkBP2jTy6T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7--Mb0lUXg6",
        "outputId": "e2f0375d-47d3-4f25-bc70-f1fdfc2948be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "# Remove the last fully connected layer\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "# Set the model to evaluation mode\n",
        "resnet.eval()\n",
        "\n",
        "# Define a function to extract features from an image\n",
        "def extract_features(image_path, model):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = preprocess(image)\n",
        "    # Add batch dimension\n",
        "    image = image.unsqueeze(0)\n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    # Remove the batch dimension\n",
        "    features = features.squeeze(0)\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "# image_path = 'example_image.png'  # Replace 'example_image.jpg' with your image path\n",
        "# features = extract_features(image_path, resnet)\n",
        "# print(features.shape)  # Output: torch.Size([2048])\n",
        "# print(features)\n",
        "\n",
        "# Save the extracted features\n",
        "torch.save(features, 'extracted_features.pt')  # Save the features to a file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3daBN_9UlG",
        "outputId": "3af23068-ae80-4105-d45d-458d6b15dad8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 111MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features using LBP"
      ],
      "metadata": {
        "id": "V6_uQaK9C9NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code can be used to extract Histogram of Gradient (HoG) Features.\n",
        "#It takes Image Path and returns HoG feature.\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_pixel(img, center, x, y):\n",
        "    new_value = 0\n",
        "    try:\n",
        "        if img[x][y] >= center:\n",
        "            new_value = 1\n",
        "    except:\n",
        "        pass\n",
        "    return new_value\n",
        "\n",
        "def lbp_calculated_pixel(img, x, y):\n",
        "    center = img[x][y]\n",
        "    val_ar = []\n",
        "    val_ar.append(get_pixel(img, center, x-1, y+1))     # top_right\n",
        "    val_ar.append(get_pixel(img, center, x, y+1))       # right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y+1))     # bottom_right\n",
        "    val_ar.append(get_pixel(img, center, x+1, y))       # bottom\n",
        "    val_ar.append(get_pixel(img, center, x+1, y-1))     # bottom_left\n",
        "    val_ar.append(get_pixel(img, center, x, y-1))       # left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y-1))     # top_left\n",
        "    val_ar.append(get_pixel(img, center, x-1, y))       # top\n",
        "\n",
        "    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "    val = 0\n",
        "    for i in range(len(val_ar)):\n",
        "        val += val_ar[i] * power_val[i]\n",
        "    return val\n",
        "\n",
        "\n",
        "def calcLBP(img):\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_tensor = torch.tensor(img_gray, dtype=torch.float32, device=device)\n",
        "\n",
        "    height, width = img_tensor.shape\n",
        "\n",
        "\n",
        "    img_lbp = torch.zeros((height, width), dtype=torch.float32, device=device)\n",
        "    for i in range(0, height):\n",
        "        for j in range(0, width):\n",
        "             img_lbp[i, j] = lbp_calculated_pixel(img_tensor, i, j)\n",
        "    # hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])\n",
        "    hist_lbp=torch.histc(img_lbp, bins=256, min=0, max=255)\n",
        "    hist_lbp /= hist_lbp.sum()\n",
        "    return(hist_lbp)\n",
        "\n",
        "\n",
        "# image_file = 'example_img.png'\n",
        "# img = cv2.imread(image_file)\n",
        "# lbpFeature=calcLBP(img)\n",
        "# print(lbpFeature)"
      ],
      "metadata": {
        "id": "zJaXZIANDQCI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features using HoG"
      ],
      "metadata": {
        "id": "-iG9Y2_0EmhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code can be used to extract Histogram of Gradient (HoG) Features.\n",
        "#It takes Image Path and returns HoG feature.\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "def compute_hog(img):\n",
        "\n",
        "  device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  img_gpu=cp.asarray(img)\n",
        "  #resizing image\n",
        "  img_gpu_resized = cp.asarray(resize(cp.asnumpy(img_gpu), (128 * 4, 64 * 4)))  # Skimage resize is CPU-only\n",
        "  # resized_img = resize(img, (128*4, 64*4))\n",
        "  #creating hog features\n",
        "  # fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
        "  #                   cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "\n",
        "  fd, hog_image = hog(cp.asnumpy(img_gpu_resized), orientations=9, pixels_per_cell=(8, 8),\n",
        "                        cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "\n",
        "  fd_tensor = torch.tensor(fd, dtype=torch.float32, device=device)\n",
        "  hog_image_tensor = torch.tensor(hog_image, dtype=torch.float32, device=device)\n",
        "\n",
        "  return fd_tensor, hog_image_tensor\n",
        "\n",
        "#reading the image\n",
        "# img = imread('example_image.png')\n",
        "# Hog_feature, hog_image=compute_hog(img)\n",
        "# print(Hog_feature.shape)"
      ],
      "metadata": {
        "id": "OUDXNcL9EqFm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining all features"
      ],
      "metadata": {
        "id": "CO_MKFsQGpPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_features=[]\n",
        "for path in paths_of_images:\n",
        "  img=cv2.imread(path)\n",
        "  cnn_features=extract_features(path, resnet)\n",
        "  lbp_features=calcLBP(img)\n",
        "  # lbp_features=torch.from_numpy(lbp_features).to(device) #Convert to tensor and move to device\n",
        "  Hog_feature, hog_image=compute_hog(img)\n",
        "  # Hog_feature = torch.from_numpy(Hog_feature).to(device) #Convert to tensor and move to device\n",
        "\n",
        "  cnn_features = cnn_features.cpu().detach().numpy().flatten()\n",
        "  lbp_features = lbp_features.reshape(-1).cpu().detach().numpy()\n",
        "  Hog_feature = Hog_feature.cpu().detach().numpy()\n",
        "\n",
        "  combined_features.append(np.concatenate((cnn_features, lbp_features, Hog_feature)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "YH0sUqiAGrC1",
        "outputId": "43b598c4-ee76-4fbf-e1a0-b7f76b0fe016"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6516cac9e2f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcnn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mlbp_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalcLBP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m# lbp_features=torch.from_numpy(lbp_features).to(device) #Convert to tensor and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mHog_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhog_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_hog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9d8bdfcfaa85>\u001b[0m in \u001b[0;36mcalcLBP\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m              \u001b[0mimg_lbp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbp_calculated_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mhist_lbp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_lbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9d8bdfcfaa85>\u001b[0m in \u001b[0;36mlbp_calculated_pixel\u001b[0;34m(img, x, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mval_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mval_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# bottom_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mval_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# bottom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mval_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# bottom_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mval_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9d8bdfcfaa85>\u001b[0m in \u001b[0;36mget_pixel\u001b[0;34m(img, center, x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_pixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from torchvision import models, transforms\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Load model on GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet = models.resnet18(pretrained=True).to(device)\n",
        "resnet.eval()\n",
        "\n",
        "# Image preprocessing for ResNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(path, model):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    img = transform(img).unsqueeze(0).to(device)  # Convert to tensor & move to GPU\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(img)\n",
        "\n",
        "    return features.cpu().detach().numpy().flatten()  # Move to CPU & flatten\n",
        "\n",
        "# GPU-accelerated LBP function\n",
        "def calcLBP(img):\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_tensor = torch.tensor(img_gray, dtype=torch.float32).to(device)\n",
        "\n",
        "    lbp_features = torch.zeros_like(img_tensor)\n",
        "    shifts = [(-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0)]\n",
        "\n",
        "    for i, (dx, dy) in enumerate(shifts):\n",
        "        shifted = torch.roll(img_tensor, shifts=(dx, dy), dims=(0, 1))\n",
        "        lbp_features += (shifted >= img_tensor) * (2 ** i)\n",
        "\n",
        "    hist_lbp = torch.histc(lbp_features, bins=256, min=0, max=256).to(device)\n",
        "    return hist_lbp\n",
        "\n",
        "# GPU-accelerated HOG\n",
        "def compute_hog(img):\n",
        "    img = resize(img, (128 * 4, 64 * 4))  # Resize\n",
        "    img_tensor = torch.tensor(img, dtype=torch.float32).to(device)\n",
        "\n",
        "    fd, hog_image = hog(img_tensor.cpu().numpy(), orientations=9, pixels_per_cell=(8, 8),\n",
        "                        cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
        "\n",
        "    return torch.tensor(fd, dtype=torch.float32).to(device), hog_image\n",
        "\n",
        "# Parallel Processing\n",
        "def process_image(path):\n",
        "    img = cv2.imread(path)\n",
        "\n",
        "    cnn_features = extract_features(path, resnet)\n",
        "    lbp_features = calcLBP(img)\n",
        "    hog_features, _ = compute_hog(img)\n",
        "\n",
        "    lbp_features = lbp_features.cpu().detach().numpy().flatten()\n",
        "    hog_features = hog_features.cpu().detach().numpy().flatten()\n",
        "\n",
        "    return np.concatenate((cnn_features, lbp_features, hog_features))\n",
        "\n",
        "# Run in parallel\n",
        "combined_features = []\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    results = list(executor.map(process_image, paths_of_images))\n",
        "\n",
        "combined_features = np.array(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvo7Q5TBpGbP",
        "outputId": "e708e288-80fc-40f9-e07c-6130d7301085"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 153MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction final"
      ],
      "metadata": {
        "id": "OFHsDQsNO_hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(combined_features)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "wYw7TvLuIRHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying PCA for dimensionality reduction"
      ],
      "metadata": {
        "id": "awUGb0PmJK8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca=PCA(n_components=50)\n",
        "\n",
        "final_features=pca.fit_transform(combined_features)\n",
        "\n",
        "print(final_features.shape)"
      ],
      "metadata": {
        "id": "Sj5t1lrSJ3AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test dataframes"
      ],
      "metadata": {
        "id": "b_c2_02Ab1sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/peopleDevTrain.csv\")\n",
        "test_df=pd.read_csv(\"/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4/peopleDevTest.csv\")\n",
        "\n",
        "train_names=train_df['name'].tolist()\n",
        "test_names=test_df['name'].tolist()\n",
        "\n",
        "train_indices=[i for i,name in enumerate(labels) if name in train_names]\n",
        "test_indices=[j for j,name in enumerate(labels) if name in test_names]\n",
        "\n",
        "X_train, X_test=final_features[train_indices], final_features[test_indices]\n",
        "y_train, y_test=labels[train_indices], labels[test_indices]\n",
        "\n",
        "print(\"Train size = \", X_train.shape)\n",
        "print(\"Test size = \", X_test.shape)\n",
        "print(\"Labels size(Train dataset)= \", y_train.shape)\n",
        "print(\"Labels size(Test dataset) = \", y_test.shape)"
      ],
      "metadata": {
        "id": "J-YMxCKyKqjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Model"
      ],
      "metadata": {
        "id": "1vd_jfouKV5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_dist(x,y):\n",
        "  return np.sqrt(np.sum(x-y)**2)"
      ],
      "metadata": {
        "id": "2sYnoEgFKlA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=3"
      ],
      "metadata": {
        "id": "VEjvfGrnm9vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=[]\n",
        "for i in range(X_test.shape[0]):\n",
        "  distances=[]\n",
        "  nearest_labels=[]\n",
        "  for j in range(X_train.shape[0]):\n",
        "    # append the pair (distance, index) to distances\n",
        "    distances.append(euclidean_dist(X_test[i], X_train[j]), j)\n",
        "  # sorting distances on the basis of first item of pair i.e. distance\n",
        "  distances.sort(key=lambda x:x[0])\n",
        "  for k in range(k):\n",
        "    nearest_labels.append(y_train[distances[k][1]])\n",
        "\n",
        "  # finding label with maximum frequency\n",
        "  max_freq_index=0\n",
        "  for label in nearest_labels:\n",
        "    freq=nearest_labels.count(label)\n",
        "    if freq>nearest_labels.count(nearest_labels[max_freq_index]):\n",
        "      max_freq_index=nearest_labels.index(label)\n",
        "\n",
        "  predictions.append(nearest_labels[max_freq_index])\n",
        "  print(X_test[i], \"\\t\", nearest_labels[max_freq_index])\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "dBOh51aIlv0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}