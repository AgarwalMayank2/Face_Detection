{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1240, 2049)\n",
      "Training size: (992, 2048), Testing size: (248, 2048)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"CNN_features_dataset.csv\", index_col=0)\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = df.iloc[:, -1].value_counts()\n",
    "\n",
    "# Filter out classes with at least 20 occurrences\n",
    "valid_classes = class_counts[class_counts >= 20].index\n",
    "df_filtered = df[df.iloc[:, -1].isin(valid_classes)]\n",
    "\n",
    "# Select exactly 20 datapoints per class\n",
    "df_limited = df_filtered.groupby(df_filtered.columns[-1]).head(20)\n",
    "\n",
    "df_limited.to_csv(\"CNN_features_dataset_limited.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1240, 2049)\n",
      "Training size: (992, 2048), Testing size: (248, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "X = df_limited.iloc[:, :-1]\n",
    "y = df_limited.iloc[:, -1]\n",
    "\n",
    "# Encode labels (alphabetically)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Ensure stratified split (16 training, 4 testing per class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=4/20, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset size: {df_limited.shape}\")\n",
    "print(f\"Training size: {X_train.shape}, Testing size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of unique values in y_train: {np.int64(0): np.int64(16), np.int64(1): np.int64(16), np.int64(2): np.int64(16), np.int64(3): np.int64(16), np.int64(4): np.int64(16), np.int64(5): np.int64(16), np.int64(6): np.int64(16), np.int64(7): np.int64(16), np.int64(8): np.int64(16), np.int64(9): np.int64(16), np.int64(10): np.int64(16), np.int64(11): np.int64(16), np.int64(12): np.int64(16), np.int64(13): np.int64(16), np.int64(14): np.int64(16), np.int64(15): np.int64(16), np.int64(16): np.int64(16), np.int64(17): np.int64(16), np.int64(18): np.int64(16), np.int64(19): np.int64(16), np.int64(20): np.int64(16), np.int64(21): np.int64(16), np.int64(22): np.int64(16), np.int64(23): np.int64(16), np.int64(24): np.int64(16), np.int64(25): np.int64(16), np.int64(26): np.int64(16), np.int64(27): np.int64(16), np.int64(28): np.int64(16), np.int64(29): np.int64(16), np.int64(30): np.int64(16), np.int64(31): np.int64(16), np.int64(32): np.int64(16), np.int64(33): np.int64(16), np.int64(34): np.int64(16), np.int64(35): np.int64(16), np.int64(36): np.int64(16), np.int64(37): np.int64(16), np.int64(38): np.int64(16), np.int64(39): np.int64(16), np.int64(40): np.int64(16), np.int64(41): np.int64(16), np.int64(42): np.int64(16), np.int64(43): np.int64(16), np.int64(44): np.int64(16), np.int64(45): np.int64(16), np.int64(46): np.int64(16), np.int64(47): np.int64(16), np.int64(48): np.int64(16), np.int64(49): np.int64(16), np.int64(50): np.int64(16), np.int64(51): np.int64(16), np.int64(52): np.int64(16), np.int64(53): np.int64(16), np.int64(54): np.int64(16), np.int64(55): np.int64(16), np.int64(56): np.int64(16), np.int64(57): np.int64(16), np.int64(58): np.int64(16), np.int64(59): np.int64(16), np.int64(60): np.int64(16), np.int64(61): np.int64(16)}\n",
      "Counts of unique values in y_test: {np.int64(0): np.int64(4), np.int64(1): np.int64(4), np.int64(2): np.int64(4), np.int64(3): np.int64(4), np.int64(4): np.int64(4), np.int64(5): np.int64(4), np.int64(6): np.int64(4), np.int64(7): np.int64(4), np.int64(8): np.int64(4), np.int64(9): np.int64(4), np.int64(10): np.int64(4), np.int64(11): np.int64(4), np.int64(12): np.int64(4), np.int64(13): np.int64(4), np.int64(14): np.int64(4), np.int64(15): np.int64(4), np.int64(16): np.int64(4), np.int64(17): np.int64(4), np.int64(18): np.int64(4), np.int64(19): np.int64(4), np.int64(20): np.int64(4), np.int64(21): np.int64(4), np.int64(22): np.int64(4), np.int64(23): np.int64(4), np.int64(24): np.int64(4), np.int64(25): np.int64(4), np.int64(26): np.int64(4), np.int64(27): np.int64(4), np.int64(28): np.int64(4), np.int64(29): np.int64(4), np.int64(30): np.int64(4), np.int64(31): np.int64(4), np.int64(32): np.int64(4), np.int64(33): np.int64(4), np.int64(34): np.int64(4), np.int64(35): np.int64(4), np.int64(36): np.int64(4), np.int64(37): np.int64(4), np.int64(38): np.int64(4), np.int64(39): np.int64(4), np.int64(40): np.int64(4), np.int64(41): np.int64(4), np.int64(42): np.int64(4), np.int64(43): np.int64(4), np.int64(44): np.int64(4), np.int64(45): np.int64(4), np.int64(46): np.int64(4), np.int64(47): np.int64(4), np.int64(48): np.int64(4), np.int64(49): np.int64(4), np.int64(50): np.int64(4), np.int64(51): np.int64(4), np.int64(52): np.int64(4), np.int64(53): np.int64(4), np.int64(54): np.int64(4), np.int64(55): np.int64(4), np.int64(56): np.int64(4), np.int64(57): np.int64(4), np.int64(58): np.int64(4), np.int64(59): np.int64(4), np.int64(60): np.int64(4), np.int64(61): np.int64(4)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get unique values and their counts in y_train and y_test\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# Create a dictionary mapping unique values to their counts\n",
    "y_train_counts = dict(zip(unique_train, counts_train))\n",
    "y_test_counts = dict(zip(unique_test, counts_test))\n",
    "\n",
    "print(\"Counts of unique values in y_train:\", y_train_counts)\n",
    "print(\"Counts of unique values in y_test:\", y_test_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alejandro_Toledo' 'Alvaro_Uribe' 'Amelie_Mauresmo' 'Andre_Agassi']\n"
     ]
    }
   ],
   "source": [
    "# Decode labels\n",
    "y_decoded = label_encoder.inverse_transform([0, 1, 2, 3])\n",
    "print(y_decoded)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRMLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
